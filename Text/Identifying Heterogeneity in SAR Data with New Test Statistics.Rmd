---
title: "Identifying Heterogeneity in SAR Data with New Test Statistics"
author:
  - name: Alejandro C. Frery
    affil: 1,*
    orcid: 0000-0002-8002-5341
  - name: Janeth Alpala
    affil: 2 
    orcid: 0000-0002-0265-6236
  - name: Abraão D. C. Nascimento
    affil: 2
    orcid: 0000-0003-2673-219X
affiliation:
  - num: 1
    address: |
      School of Mathematics and Statistics, Victoria University of Wellington, Wellington 6140, New Zealand
#email: alejandro.frery@vuw.ac.nz
  - num: 2
    address: |
      Departamento de Estatística, Universidade Federal de Pernambuco, Recife 50670-901, PE, Brazil
#email: janeth.alpala@ufpe.br, abraao@de.ufpe.br 
    
# author citation list in chicago format
authorcitation: |
  Frery, A.C.; Alpala, J.; Nascimento,A.D.C.
# firstnote to eighthnote
#firstnote: |
 # Current address: Updated affiliation
#secondnote: |
  #These authors contributed equally to this work.
correspondence: |
  alejandro.frery@vuw.ac.nz
# document options
journal: remotesensing
type: article
status: submit
# front matter
#simplesummary: |
  #A Simple summary goes here.
abstract: |
  This paper presents a statistical approach to identify the underlying roughness characteristics in synthetic aperture radar (SAR) intensity data. The physical modeling of this kind of data allows to use the Gamma distribution in the presence of fully-developed speckle, i.e., when there are infinitely many independent backscatterers per resolution cell. This situation is often called "homogeneous" or 
  "textureless" areas. The $\mathcal{G}_I^0$ distribution is also a widely accepted law for heterogeneous and extremely heterogeneous areas. We propose three test statistics for discriminating between homogeneous and non-homogeneous areas, i.e., between Gamma and GI0 distributed data, both with a known number of looks. The first test statistic utilizes a bootstrapped non-parametric estimator of the Shannon entropy, offering a robust evaluation in the face of uncertain distributional assumptions. The second test uses the classic coefficient of variation. The third test employs an alternative form of estimating the CV based on the ratio of the median absolute deviation to the median. We apply our test statistic to produce maps of $p$-values for the hypothesis of homogeneity. These maps identify different types of targets. We show that our proposal outperforms its competitors using simulated and SAR data.
# back matter
keywords: |
  SAR; heterogeneity; entropy;  coefficient of variation; hypothesis tests
acknowledgement: |
  This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001.
authorcontributions: |
  For research articles with several authors, a short paragraph specifying their 
  individual contributions must be provided. The following statements should be 
  used ``X.X. and Y.Y. conceive and designed the experiments; X.X. performed the 
  experiments; X.X. and Y.Y. analyzed the data; W.W. contributed 
  reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be
  limited to those who have contributed substantially to the work reported.
funding: |
  Please add: "This research received no external funding" or "This research 
  was funded by NAME OF FUNDER grant number XXX."  and ``The APC was funded 
  by XXX''. Check carefully that the details given are accurate and use the 
  standard spelling of funding agency names at 
  \url{https://search.crossref.org/funding}, any errors may affect your future 
  funding.
institutionalreview: |
 Not applicable.
#informedconsent: |
  # Any research article describing a study involving humans should contain this 
  # statement. Please add ``Informed consent was obtained from all subjects 
  # involved in the study.'' OR ``Patient consent was waived due to REASON 
  # (please provide a detailed justification).'' OR ``Not applicable'' for 
  # studies not involving humans. You might also choose to exclude this statement 
  # if the study did not involve humans.
  
  # Written informed consent for publication must be obtained from participating 
  # patients who can be identified (including by the patients themselves). Please 
  # state ``Written informed consent has been obtained from the patient(s) to 
  # publish this paper'' if applicable.
dataavailability: |
  Not applicable.
conflictsofinterest: |
  The authors declare no conflict of interest.
supplementary: |
 This article is fully reproducible using RMarkdown. All code and data are accessible at
 \url{https://github.com/rjaneth/Janeth-Alpala} (accessed on 10 March 2024).
abbreviations:
  - short: MDPI
    long: Multidisciplinary Digital Publishing Institute
  - short: DOAJ
    long: Directory of open access journals
  - short: TLA
    long: Three letter acronym
  - short: LD 
    long: linear dichroism
    
header-includes:
   - \usepackage[english]{babel}
   - \usepackage{bm,bbm}
   - \usepackage{mathrsfs}
   - \usepackage{siunitx}
   - \usepackage{graphicx}
   - \usepackage{url}
   - \usepackage[T1]{fontenc}
   - \usepackage{polski}
   - \usepackage{booktabs}
   - \usepackage{color}
   - \usepackage{amsmath}
   
bibliography: references.bib
#appendix: appendix.tex
endnotes: false
output: 
  rticles::mdpi_article:
    extra_dependencies: longtable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE)
#knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(reshape2)
#library(plotly)
library(knitr)
library(pandoc)
library(gridExtra)
library(gtools)
library(stats4)
library(rmutil)
library(scales)
library(tidyr)
library(rmutil)
library(invgamma)
library(tidyverse)
library(RColorBrewer)
library(ggsci)
library(ggpubr)
library(patchwork)
library(dplyr)
#options(kableExtra.latex.load_packages = FALSE)
#library(devtools)
#devtools::install_github("haozhu233/kableExtra")
library(kableExtra)
library(ggthemes)
library(latex2exp)
library(e1071)# kurtosis
library(viridis)
library(nortest)# AD




theme_set(theme_minimal() +
            theme(text=element_text(family="serif"),
                  legend.position = "bottom")#  top , right , bottom , or left#, panel.grid = element_blank()
)

# if(!require("rstudioapi")) install("rstudioapi")
# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# opts_chunk$set(comment="",
#                message=FALSE,
#                tidy.opts=list(keep.blank.line=TRUE, width.cutoff=120),
#                options(width=100), 
#                # cache=TRUE,
#                fig.align='center',
#                fig.height=6, fig.width=10,
#                fig.show='hold',
#                cache=TRUE)


source("../Code/R/MainFunctions/gamma_sar_sample.R")
source("../Code/R/MainFunctions/entropy_gamma_sar.R")
source("../Code/R/MainFunctions/entropy_gI0.R")
source("../Code/R/MainFunctions/gi0_sample.R")

source("../Code/R/MainFunctions/van_es_estimator.R")
source("../Code/R/MainFunctions/correa_estimator.R")
source("../Code/R/MainFunctions/bootstrap_correa_estimator_log_mean.R")
source("../Code/R/MainFunctions/ebrahimi_estimator.R")
source("../Code/R/MainFunctions/noughabi_arghami_estimator.R")
source("../Code/R/MainFunctions/vasicek_estimator.R")
source("../Code/R/MainFunctions/al_omari_1_estimator.R")
source("../Code/R/MainFunctions/al_omari_2_estimator.R")

source("../Code/R/MainFunctions/bootstrap_van_es_estimator.R")
source("../Code/R/MainFunctions/bootstrap_correa_estimator.R")
source("../Code/R/MainFunctions/bootstrap_ebrahimi_estimator.R")
source("../Code/R/MainFunctions/bootstrap_noughabi_arghami_estimator.R")
source("../Code/R/MainFunctions/bootstrap_vasicek_estimator.R")
source("../Code/R/MainFunctions/bootstrap_al_omari_1_estimator.R")
source("../Code/R/MainFunctions/bootstrap_al_omari_2_estimator.R")
#The next function contains the functions: generate_samples, calculate_bias_mse, generate_plot
source("../Code/R/Programs/functions_sample_bias_mse.R")# read_ENVI_images
source("../Code/R/Programs/read_ENVI_images.R")


```

```{r DefinitionSimulationRoutines, echo=FALSE}
#set.seed(1234567890, kind="Mersenne-Twister")
```
\setlength{\tabcolsep}{6pt} 
\newcommand{\bias}{\operatorname{Bias}}
\newcommand{\widebar}[1]{\overline{#1}}

# Introduction {#sec:Introduction}

Synthetic Aperture Radar (SAR) technology has become an important tool for environmental monitoring and disaster management, providing valuable imagery in different conditions, including day or night and diverse weather situations&nbsp;\cite{Mu2019}.
However, the effective utilization of SAR data depends on a thorough understanding of its statistical properties, especially the impact of speckle effects, which are inherent in SAR data due to the coherent nature of the imaging process.

In the presence of non-Gaussian speckle interference, SAR data requires a reliable statistical model for accurate processing. The $\mathcal{G}^0$ distribution, suitable for SAR data, includes the Gamma law as a specific case for fully developed speckle, providing flexibility with fewer parameters for analysis.

Our work is motivated to improve the identification of potential roughness features in SAR intensity data. 
Physical modeling of SAR data allows to take advantage of the gamma distribution in the presence of fully developed speckle, creating a scenario with an infinite number of independent backscatterers per unit of resolution, often referred to as homogeneous areas.

In this context, we present a set of three novel test statistics designed to discriminate between homogeneous and non-homogeneous areas, specifically aimed at differentiating between Gamma and $\mathcal{G}^0$ distributed data, each with a known number of appearances. 
Hence, to understand the core ideas behind our approach, we explore into the use of properties such as entropy and coefficient of variation.

The entropy is a fundamental concept in information theory with broad applications to pattern recognition, statistical physics, stochastic dynamics, and statistics. 
Shannon introduced it for a random variable in 1948&nbsp;\cite{Shannon1948} as a measure of information and uncertainty.  
In statistics, Shannon entropy is a crucial descriptive parameter, particularly for assessing data dispersion and conducting tests for normality, exponentiality, and uniformity&nbsp;\cite{Wieczorkowski1999}. 
Entropy estimation involves several practical difficulties, especially when the model is unknown; in these cases, non-parametric methods are used.
Among the non-parametric approaches, Subhash et al.&nbsp;\cite{Subhash2021} discussed the use of spacing methods.
This non-parametric strategy offers flexibility to address a wide range of models without imposing specific parametric restrictions.
In our study, entropy serves as the fundamental parameter, providing a robust assessment by means of a bootstrapped non-parametric estimator of Shannon entropy.


Simultaneously, we used the classic coefficient of variation (CV) and an alternative form based on the ratio between the median absolute deviation and the median.
The coefficient of variation, recognised as an efficient and robust index of textural information, measures the homogeneity of the image.

Applying these test statistics to generate $p$-value maps facilitates testing of the homogeneity hypothesis, revealing diverse types of targets within the SAR data. 
Through extensive experimentation involving real and simulated SAR data, we demonstrate the superiority of our proposed methodology over existing approaches.

The article is structured as follows: 
Section&nbsp;\ref{sec:Background} covers statistical modeling and entropy estimation for Intensity SAR data. Section&nbsp;\ref{sec:test} outlines hypothesis testing based on non-parametric entropy estimators and and coefficient of variation. 
In Section&nbsp;\ref{sec:results}, we present experimental results. 
Finally, in Section&nbsp;\ref{sec:conclusion} conclusions are exhibited.

# Background {#sec:Background} 

## Statistical modeling of Intensity SAR data 

The primary models used for intensity SAR data include the Gamma and $\mathcal{G}_I^0$  distributions&nbsp;[@Frery1997]. 
The first is suitable for fully developed speckle and is a limiting case of the second, which is appealing due to its versatility in accurately representing regions with various roughness characteristics&nbsp;[@Cassetti2022].
We denote $Z \sim \Gamma_{\text{SAR}}(L, \mu)$ and $Z \sim G_I^0(\alpha, \gamma, L)$ to indicate that $Z$ follows the distributions characterized by the respective probability density functions:
\begin{align}
	f_Z(z;L, \mu)&=\frac{L^L}{\Gamma(L)\mu^L}z^{L-1}\exp\left\{-Lz/\mu\right\} \mathbbm 1_{\mathbbm R_+}(z),\label{E:gamma1}\\
	f_Z(z; \alpha, \gamma, L)&=\frac{L^L\Gamma(L-\alpha)}{\gamma^{\alpha}\Gamma(-\alpha)\Gamma(L)}\cdot\frac{z^{L-1}}{(\gamma+Lz)^{L-\alpha}} \mathbbm 1_{\mathbbm R_+}(z),\label{E:gi01}
\end{align}
where, in&nbsp;\eqref{E:gamma1} $\mu > 0$ is the mean; in&nbsp;\eqref{E:gi01}  $\gamma > 0$ is the scale, $\alpha < -1$ measures the roughness,  $L \geq 1$ is the number of looks, $\Gamma(\cdot)$ is the gamma function, and $\mathbbm 1_{A}(z)$ is the indicator function of the set $A$.

From \eqref{E:gi01}, the $r$th moment of $Z$ is expressed as:
\begin{align}
	E_{G_I^0}\left(Z^r\right)=\left(\frac{\gamma}{L}\right)^r\frac{\Gamma(-\alpha-r)}{\Gamma(-\alpha)}\cdot\frac{\Gamma(L+r)}{\gamma(L)}, \quad \alpha <-r. 
	\label{E:rmom}
\end{align}
 
Even though the $\mathcal{G}_I^0$  distribution is defined by the parameters $\alpha$ and $\gamma$, SAR literature commonly utilizes the texture $\alpha$ and the mean $\mu$&nbsp;\cite{Nascimento2010}.
In this way, we compute the expected value $\mu$ using the expression in&nbsp;\eqref{E:rmom}, and we reparametrize&nbsp;\eqref{E:gi01} using $\mu$, $\alpha$, and $L$. Then
\begin{align*}
	\mu=\left(\frac{\gamma}{L}\right)\frac{\Gamma(-\alpha-1)}{\Gamma(-\alpha)}\cdot\frac{\Gamma(L+1)}{\gamma(L)}=-\frac{\gamma}{\alpha+1}.
\end{align*}
Thus, the probability density function that characterize the $G_I^0(\mu, \alpha, L)$ law is
\begin{equation}
		f_Z(z; \mu, \alpha, L)=\frac{L^L\Gamma(L-\alpha)}{\big(-\mu(\alpha+1)\big)^{\alpha}\Gamma(-\alpha)\Gamma(L)} \frac{z^{L-1}}{\big(-\mu(\alpha+1)+Lz\big)^{L-\alpha}}.\label{E:gi02}
\end{equation}

## The Shannon Entropy
The parametric representation of Shannon entropy for a system described by a continuous random variable is:
\begin{equation}
  \label{E:entropy2}
  H(Z)=-\int_{-\infty }^\infty \ f(z)\ln f(z)\, \mathrm{d}z,
\end{equation}
here, $f(\cdot)$ is the probability density function that characterizes the distribution of the real-valued random variable $Z$.

Using&nbsp;\eqref{E:entropy2}, we can express the Shannon entropy of $\Gamma_{\text{SAR}}$in&nbsp;\eqref{E:gamma1} and $G_I^0$ in&nbsp;\eqref{E:gi02} based on&nbsp;\cite{Cassetti2022} and&nbsp;\cite{Ferreira2020}:
\begin{equation}
\label{E:E-gamma}
H_{\Gamma_{\text{SAR}}}(L, \mu) =   L -\ln L+\ln\Gamma(L)+(1-L)\psi^{(0)}(L) + \ln \mu, 
\end{equation}
\begin{multline}
\label{E:E-GIO}
H_{G_I^0}(\mu, \alpha, L) =L -\ln L+\ln\Gamma(L)+(1-L)\psi^{(0)}(L) +\ln \mu -\ln\Gamma(L-\alpha)\\
+ (L-\alpha) \psi^{(0)}(L-\alpha)-(1-\alpha)\psi^{(0)}(-\alpha)+\ln (-1-\alpha)+\ln\Gamma(-\alpha)-L,
\end{multline}
where $\psi^{(0)}(\cdot)$ is the digamma function. Fig.&nbsp;\ref{fig:Plot_GI0_to_gamma1}, shows the behaviour of the entropy of \(G_I^0\) when when \(\alpha \in \left\{-\infty, -20, -8, -3\right\}\), observing its convergence towards the entropy of \(\Gamma_{\text{SAR}}\) as \(\alpha\) takes large negative values. 

```{r Plot_GI0_to_gamma1, echo=FALSE, message=FALSE, warning=FALSE, out.width="70%", fig.show="hold", fig.pos="hbt", fig.cap="$H_{ G_I^0}$ converges to the $H_{\\Gamma_{\\text{SAR}}}$ with $L=8$."}



L <- c(8)
alphas <- c(  -3, -8, -20, -1000)
alpha_labels <- c( expression(italic(alpha) == -3), expression(italic(alpha) == -8), expression(italic(alpha) == -20), expression(italic(alpha) == -1000))

mu <- seq(0.1, 10, length.out = 500)

# Entropy GI0

muEntropy <- data.frame()

for (alpha in alphas) {
  entropies_GI0 <- sapply(L, function(L) entropy_gI0(mu, alpha, L))
  muEntropy <- rbind(muEntropy, data.frame(mu = mu, Entropy = entropies_GI0, alpha = as.factor(alpha)))
}

muEntropy.molten <- melt(muEntropy, id.vars = c("mu", "alpha"),  value.name = "Entropy")

# Entropy Gamma SAR

entropies_gamma <- sapply(L, function(L) entropy_gamma_sar(L, mu))

Entropy_gamma <- data.frame(mu, entropies_gamma)

Entropy_gamma.molten <- melt(Entropy_gamma, id.vars = "mu", value.name = "Entropy Gamma")

#plot

ggplot(muEntropy.molten, aes(x = mu, y = Entropy, col = alpha)) +
  geom_line(data = Entropy_gamma.molten, aes(x = mu, y = `Entropy Gamma`), color = "blue", linetype = "solid",linewidth = 1.5) + 
  geom_line(linetype = "longdash",  linewidth = 2, alpha=.7) +
  annotate("text", x = max(mu)+0.2, y = max(Entropy_gamma.molten$`Entropy Gamma`), 
           label = TeX("$\\Gamma_{\\tiny{SAR}}$"), vjust = 0.9, hjust = 0.1, color = "blue")+
  theme_minimal() +
  scale_color_manual(values = brewer.pal(7, "Dark2")[1:5], labels = alpha_labels) +
  #scale_color_manual(values = pal_jama()(7)[2:5], labels = alpha_labels) +
  #scale_color_manual(values = 	pal_cosmic()(7)[1:5], labels = alpha_labels) +
  labs(col = "Roughness", linetype = NULL) +
  xlab(expression(paste(mu))) +
  ylab("Entropy") +  
  theme(text = element_text(family = "serif"),
        legend.position = "bottom")


```

## Estimation of the Shannon Entropy

The problem of non-parametric estimating of $H(Z)$ has been considered by many authors including&nbsp;\cite{vasicek1976test, Wieczorkowski1999,correa1995new,AlOmari2019}, who proposed estimators based on spacings.

One of the earliest non-parametric estimators relying on spacings was introduced by Vasicek&nbsp;\cite{vasicek1976test}. 
Assuming that $\bm{Z}=(Z_1, Z_2,\ldots,Z_n)$ is a random sample from the distribution $F(z)$, the estimator is defined as:
\begin{equation*}
\label{E:Vas}
	\widehat{H}_{\text{V}}(\bm{Z})=\frac{1}{n}\sum_{i=1}^{n}\ln\left[\frac{n}{2m}\left(Z_{(i+m)}-Z_{(i-m)}\right)\right],
	\end{equation*}
where $m<n/2$ is a positive integer, $Z_{(i+m)}-Z_{(i-m)}$ is the $m$-spacing and $Z_{(1)}\leq Z_{(2)}\leq\ldots\leq Z_{(n)}$ are the order statistics and $Z_{(i)}= Z_{(1)}$ if $i<1$, $Z_{(i)}= Z_{(n)}$ if $i>n$.

Several authors have explored adaptations to Vasicek's estimator. 
We consider three estimators known for their superior performance&nbsp;\cite{Cassetti2022}:

* Correa&nbsp;\cite{correa1995new}: $\widehat{H}_{\text{C}}$.

* Ebrahimi et al&nbsp;\cite{Ebrahimi1994}: $\widehat{H}_{\mathrm{E}}$.

* Al Omary&nbsp;\cite{IbrahimAlOmari2014}: $\widehat{H}_{\mathrm{AO}}$.

## Enhanced Bootstrap Technique

We employ the bootstrap technique to refine the precision of existing non-parametric entropy estimators. 
This approach involves generating new datasets through resampling with repetition from an existing one&nbsp;\cite{Michelucci2021}.

 
<!-- This approach involves generating new datasets through resampling with repetition from an existing one. -->

Let's assume that non-parametric entropy estimators \(\widehat{H}=\widehat{\theta}(\bm{Z})\) are inherently biased, that is:
\begin{equation}
\label{Eq:bias1}
\bias\big(\widehat{\theta}(\bm{Z})\big) = E\big[\widehat{\theta}(\bm{Z})\big] - \theta \neq 0.
\end{equation}
Our bootstrap-improved estimators are of the form:
<!-- introducing an "ideal unbiased estimator" \(\check{\theta}(\bm{Z})\): -->
<!-- \begin{equation} -->
<!-- \label{Eq:bias2} -->
<!-- \widecheck{\theta}(\bm{Z}) = \widehat{\theta}(\bm{Z}) - \bias\big(\widehat{\theta}(\bm{Z})\big). -->
<!-- \end{equation} -->
<!-- However, \(\check{\theta}(\bm{Z})\) is not an estimator, because it depends on the true parameter \(\theta\), prompting the formulation of a new estimator \(\widetilde{H}\).  -->
<!-- From&nbsp;\eqref{Eq:bias1} and&nbsp;\eqref{Eq:bias2} we have: -->
\begin{align*}
\widetilde{H} &= 2\widehat{\theta}(\bm{Z}) - \frac{1}{B}\sum_{b=1}^B \widehat{\theta}_b(\bm{Z}^{(b)}),
\end{align*}
where $B$ is the number of replications in the bootstrap technique.
Applying this methodology, the original estimators by Correa, Ebrahimi, and Al-Omari are now denoted as the proposed bootstrap-enhanced versions: $\widetilde{H}_{\text{C}}$, $\widetilde{H}_{\text{E}}$, and $\widetilde{H}_{\text{AO}}$, respectively.


We analyzed the performance of these estimators with a Monte Carlo study:
$500$ samples of $G_I^0$ of size \(n\in\left\{9, 25, 49, 81, 121\right\}\) and several values of the mean, number of looks and roughness.
We present the results with \(mu\in\left\{1, 10\right\}\), \(\alpha=-10\), and \(L=5\). 
The results are consistent for the other situations. 
In the case of the bootstrap technique, each sample is replicated $100$ times with replacement.
We choose to use the following heuristic formula for spacing, $m=\left[\sqrt{n}+0.5\right]$.

In Fig.&nbsp;\ref{fig:Plot_bias_mse_gi0} we depict comparisons of bias and mean squared error (MSE) between the original non-parametric entropy estimators and their respective bootstrap-enhanced versions. 
The use of the bootstrap technique exhibits more precision, reduced bias and MSE, and improved convergence.
The results of simulation are exhibited in Table&nbsp;\ref{tab:table2}.

The precision of estimators, as evidenced by bias and MSE comparisons, benefits significantly from the bootstrap technique, particularly for sample sizes below 81.


```{r Simulated_data_gi0, echo=FALSE, message=FALSE}

set.seed(1234567890, kind = "Mersenne-Twister")
sample_sizes <- c(9, 25, 49, 81, 121 )

# Number of replications
R <-200

# Number of bootstrap replications
B1 <- 5
mu_values <- c(1, 10)
alpha <- -20
L <- 5

estimators <- list(
  "Correa" = correa_estimator,
  "Ebrahimi" = ebrahimi_estimator,
  "Al Omari" = al_omari_1_estimator,
  "Correa Bootstrap" = bootstrap_correa_estimator,
  "Ebrahimi Bootstrap" = bootstrap_ebrahimi_estimator,
  "Al Omari Bootstrap" = bootstrap_al_omari_1_estimator
)


calculate_results_gi0 <- function(sample_sizes, R, B1, mu_values, alpha, L, estimators) {
  results_list <- list()

  for (mu_val in mu_values) {
    
    results <- calculate_bias_mse_gi0(sample_sizes, R, B1, mu_val, alpha, L, estimators)
    df <- as.data.frame(results)

    
    results_list[[as.character(mu_val)]] <- df
  }

  return(results_list)
}


results_gi0 <- calculate_results_gi0(sample_sizes, R, B1, mu_values, alpha, L, estimators)


save(results_gi0, file = "./Data/resultsgi0_1.Rdata")


```


```{r Plot_bias_mse_gi0, echo=FALSE, message=FALSE, warning=FALSE, out.width="80%", fig.pos="hbt",  fig.cap="Bias and MSE, $L=5$ and  $\\alpha=-10$.", fig.width=8, fig.height=5.5}


load("./Data/resultsgi0_1.Rdata")


estimators_to_plot <- c("Correa", "Ebrahimi", "Al Omari",  "Correa Bootstrap", "Ebrahimi Bootstrap", "Al Omari Bootstrap" )
  latex_estimator_names <- c("Correa" = expression("$\\widehat{italic(H)}_{C}$"),
                             "Correa Bootstrap" = expression("$\\widetilde{italic(H)}_{C}$"),
                            "Ebrahimi" = expression("$\\widehat{italic(H)}_{E}$"),
                            "Al Omari" = expression("$\\widehat{italic(H)}_{AO}$"),
                            "Ebrahimi Bootstrap" = expression("$\\widetilde{italic(H)}_{E}$"),
                            "Al Omari Bootstrap" = expression("$\\widetilde{italic(H)}_{AO}$"))
selected_estimators_latex <- latex_estimator_names[estimators_to_plot]


combined_plot_gi0 <- generate_plot_gi0_esp(results_gi0, mu_values, selected_estimators_latex, ncol = 1, nrow = 2)


print(combined_plot_gi0)


```



```{r Table_gi0, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#cat("\\setlength{\\tabcolsep}{0.5pt}\n")
load("./Data/resultsgi0_1.Rdata")

# Define estimators
estimators_to_table <- c( "Al Omari","Correa", "Ebrahimi", "Al Omari Bootstrap",  "Correa Bootstrap","Ebrahimi Bootstrap")

# Filter and reshape data
filtered_results <- purrr::map_dfr(results_gi0, ~ .x %>% filter(Estimator %in% estimators_to_table), .id = "mu")

reshaped_results <- filtered_results %>%
  pivot_wider(names_from = Estimator, values_from = c(Bias, MSE))

colnames(reshaped_results) <- c("$\\mu$", "$n$", "$\\widehat{H}_{\\text{AO}}$", "$\\widehat{H}_{\\text{C}}$", "$\\widehat{H}_{\\text{E}}$", "$\\widetilde{H}_{\\text{AO}}$", "$\\widetilde{H}_{\\text{C}}$", "$\\widetilde{H}_{\\text{E}}$",  "$\\widehat{H}_{\\text{AO}}$", "$\\widehat{H}_{\\text{C}}$", "$\\widehat{H}_{\\text{E}}$","$\\widetilde{H}_{\\text{AO}}$", "$\\widetilde{H}_{\\text{C}}$", "$\\widetilde{H}_{\\text{E}}$")


reshaped_results[] <- lapply(reshaped_results, function(x, col_name) {
  if (is.numeric(x)) {
    if (col_name == "\\mu") {
      formatted_numbers <- sprintf("$\\scriptstyle %d$", x)
    } else {
      if (all(x %% 1 == 0)) {
        formatted_numbers <- sprintf("$%d$", x)
      } else {
        formatted_numbers <- ifelse(x < 0, sprintf("$%.3f$", x), sprintf("$\\phantom{-}%.3f$", x))
      }
    }
    return(formatted_numbers)
  } else {
    return(x)
  }
}, col_name = names(reshaped_results)[1])  


#print(
 # kbl(
knitr::kable(
    reshaped_results,
    caption = "Bias and MSE of non-parametric entropy estimators and bootstrap versions.",
    format = "latex",
    booktabs = TRUE,
    align = "lrrrrrrrrrrrrrrr",
    escape = FALSE, 
    digits = 2,  
    label = "table2",
    centering = FALSE,
    table.envir = "table", position = "H") %>%
    add_header_above(c(" ", " ", "Bias" = 6,  "MSE" = 6)) %>%
    collapse_rows(columns = 1:2, latex_hline = "major", valign = "middle") %>%
    row_spec(0,  align = "c")%>%
    kable_styling(latex_options = "scale_down")
  #)
  #kable_styling(latex_options = c("scale_down"))

```

# Hypothesis testing based on non-parametric entropy {#sec:test}

General asymptotic results for functions of spacings are detailed in Ref.&nbsp;\cite{Khashimov1990}, while Van Es&nbsp;\cite{Bert1992} developed a correction for the case of Shannon entropy.
Following the work of these authors, the next result applies: 
<!-- \begin{lemma} -->
<!-- Suppose that $f(\cdot)$ is a bounded density bounded away from zero and satisfies a Lipschitz condition on its support. -->
<!-- Then, if $m,n\rightarrow \infty$ and $m=o(n^{1/2})$, one holds that: -->
<!-- \begin{equation*} -->
<!-- \sqrt{n}\,\Big(\label{Eq:bias_t} -->
<!-- \widetilde{H}_{i}+\int_{-\infty}^\infty f(z)\ln f(z) \mathrm{d}z\Big) -->
<!-- \xrightarrow[]{\mathcal{D}} -->
<!-- \mathcal{N}\big(0,\operatorname{Var}(\ln f(Z))\big), -->
<!-- \end{equation*} -->
<!-- where $o(\cdot)$ represents the little-o. -->
<!-- \end{lemma}  -->


We aim at testing the following hypotheses: 

$$
 \begin{cases}\mathcal{H}_0: \widetilde{H}= H_{\Gamma_{\text{SAR}}}\\ 
  \mathcal{H}_1:\widetilde{H}\neq H_{\Gamma_{\text{SAR}}}.\end{cases}
$$
In other words, we verify the hypothesis that the data are fully-developed speckle with the following test statistic:
\begin{equation}
\label{Eq:test_e}
S(\bm{Z};L)= \widetilde{H}-\left[H_{\Gamma_{\text{SAR}}}(L)+\ln \widebar{\bm{Z}}\right].
\end{equation}
<!-- This test statistic assess the behavior of the data under the null hypothesis, through the empirical distribution.  -->
The values should be around zero under the null hypothesis and far otherwise. 
<!-- This suggests significant differences and implies the presence of heterogeneous clutter. -->

# Results {#sec:results} 


## The Proposed Test
Fig.&nbsp;\ref{fig:Plot_density} depicts empirical test statistic densities for various sample sizes, and Table&nbsp;\ref{tab:table_stat_combined} summarizes key statistics, including mean, SD, variance (Var), skewness (SK), excess kurtosis (EK), and Anderson–Darling $p$-values for normality. 
<!-- ACF Only show L=2 and L=8 -->
Results with $p$-values larger than $0.05$  indicate no evidence against normality. 
Small variance and near-zero skewness and excess kurtosis suggest limited spread, symmetry, and light tails.  
Normal QQ-plots, confirm that there is no evidence to reject the normal distribution.

<!-- In Fig.&nbsp;\ref{fig:Plot_normality_qq}, normal QQ-plots visually affirm distributional similarity with the expected normal line. -->

```{r Simulated_density, echo=FALSE, message=FALSE}
#, cache = TRUE, autodep = TRUE
set.seed(1234567890, kind = "Mersenne-Twister")

R <- 500
mu <- 1
B <- 1

sample.size <- c(25,49, 81, 121)
L_values <- c(3,  5, 8, 11)

all_summary_stats <- list()
all_TestStatistics <- list()


# For each L
for (L in L_values) {
  TestStatistics1 <- list()  
  summary_stats <- data.frame( LValue = character(),
                              SampleSize = numeric(),
                              Mean = numeric(),
                              SD = numeric(),
                              Variance = numeric(),
                              #CV= numeric(),
                              Skewness = numeric(),
                              Kurtosis = numeric(),
                              adpvalue = numeric()
                              )  
 
  
  for (s in sample.size) {
    TestStat1 <- numeric(R)
    
    for (r in 1:R) {
      z <- gamma_sar_sample(L, mu, s)
      #z <- gi0_sample(mu, alpha1, L, s)
      TestStat1[r] <- bootstrap_correa_estimator_log_mean(z, B) + (-L + log(L) - lgamma(L) - (1 - L) * digamma(L))
    }
    
    TestStatistics1[[as.character(s)]] <- data.frame("SampleSize" = rep(s, R), "Test_Statistics" = TestStat1)
    

    
   
    mean_val <- mean(TestStat1)
    sd_val <- sd(TestStat1)
    var_val <- var(TestStat1)
    skewness_val <- skewness(TestStat1)
    kurtosis_val <- kurtosis(TestStat1)
   # lillie_p_value <- lillie.test(TestStatistics1[[as.character(s)]]$Test_Statistics)$p.value
    ad_p_value <- ad.test(TestStatistics1[[as.character(s)]]$Test_Statistics)$p.value
    #cv_val <- abs(sd_val / mean_val)
    
    summary_stats <- rbind(summary_stats, data.frame(LValue = as.character(L),
                                                     SampleSize = s,
                                                     Mean = mean_val,
                                                     SD = sd_val,
                                                     Variance = var_val,
                                                      #CV = cv_val,
                                                     Skewness = skewness_val,
                                                     Kurtosis = kurtosis_val,
                                                     adpvalue = ad_p_value
                                                     )) 
  
  }
  
  all_TestStatistics[[as.character(L)]] <- TestStatistics1
  all_summary_stats[[as.character(L)]] <- summary_stats
  
  

  save(all_TestStatistics, all_summary_stats, file = paste0("./Data/results1_", L, ".Rdata"))
}




```

```{r Plot_density, echo=FALSE, message=FALSE, warning=FALSE, out.width="80%", fig.pos="hbt",  fig.cap="Empirical density  under null hypothesis.", fig.width=6, fig.height=5.0}

theme_set(theme_minimal() +
            theme(text = element_text(family = "serif"),
                  legend.position = "bottom",
                  legend.text = element_text(angle = 0, vjust = 0.5)))

all_plots <- list()

for (L in L_values) {
  load(paste0("./Data/results1_", L, ".Rdata"))

  combined_data <- do.call(rbind, all_TestStatistics[[as.character(L)]])
  combined_data$L <- L

  p <- ggplot(combined_data, aes(x = Test_Statistics, col = factor(SampleSize), linetype = factor(SampleSize))) +
    geom_line(stat = "density", linewidth = 1.0) +
    scale_color_viridis(discrete = TRUE, option = "C", direction = -1, begin = 0.1, end = 0.8, name = "Sample Size") +
    scale_linetype_manual(values = rep("solid", length(sample.size)), name = "Sample Size") +
    labs(x = "Test Statistics", y = "Density") +
    ggtitle(bquote(italic(L) == .(L))) +
    theme(plot.title = element_text(hjust = 0.5))#titulo centrado

  if (!any(L == L_values)) {
    p = p
  }

  all_plots[[as.character(L)]] <- p
}

combined_plot <- wrap_plots(all_plots, ncol = 2, nrow = 2) +
  plot_layout(guides = "collect")

print(combined_plot)



```


```{r Table_statistic, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

#cat("\\setlength{\\tabcolsep}{1.0pt}\n")

load("./Data/results1_3.Rdata")
summary_stats_2 <- all_summary_stats[["3"]]


load("./Data/results1_5.Rdata")
summary_stats_5 <- all_summary_stats[["5"]]


load("./Data/results1_8.Rdata")
summary_stats_8 <- all_summary_stats[["8"]]


combined_summary_stats <- rbind(
  transform(summary_stats_2),
  transform(summary_stats_5),
  transform(summary_stats_8)
  
)
colnames(combined_summary_stats) <- c("$L$", "$n$", "Mean", "SD", "Var", "SK", "EK", "$p$-value")



# LaTeX notation 
combined_summary_stats[] <- lapply(combined_summary_stats, function(x) {
  if (is.numeric(x)) {
    if (all(x %% 1 == 0)) {
      formatted_numbers <- sprintf("$%d$", x)
    } else {
      formatted_numbers <- ifelse(x < 0, sprintf("$%.4f$", x), sprintf("$\\phantom{-}%.4f$", x))
    }
    return(formatted_numbers)
  } else {
    return(x)
  }
})


table_result_combined <- knitr::kable(
  combined_summary_stats,
  caption = "Descriptive analysis of $S(\\bm Z; L)$ with $L\\in\\left\\{3,5, 8\\right\\}$ and $\\mu=1$.",
  format = "latex",
  booktabs = TRUE,
  align = "lrrrrrrr",
  escape = FALSE,
  digits = 4,
  label = "table_stat_combined",
  centering = FALSE,
  table.envir = "table", position = "H") %>%
  #table.env = 'table*' # to span multiple columns
  collapse_rows(latex_hline = "major", valign = "middle") %>%
  row_spec(0,  align = "c")
  

print(table_result_combined)


```

```{r Plot_normality_qq, echo=FALSE, message=FALSE, warning=FALSE, out.width="80%", fig.pos="hbt",  fig.cap="Normal QQ-plots for  $n=121$.", fig.width=6, fig.height=5}



theme_set(theme_minimal() +
            theme(text = element_text(family = "serif"),
                  legend.position = "bottom",
                  legend.text = element_text(angle = 0, vjust = 0.5)))



# Load saved results
load("./Data/results1_3.Rdata")
load("./Data/results1_5.Rdata")
load("./Data/results1_8.Rdata")
load("./Data/results1_11.Rdata")

# Specific sample size
selected_sample_size <- 121

# 
ggplot_list <- list()

#  L
for (L in c(3, 5, 8, 11)) {
  TestStatistics1 <- all_TestStatistics[[as.character(L)]]
  
  # 
  if (as.character(selected_sample_size) %in% names(TestStatistics1)) {
    p <- ggplot(TestStatistics1[[as.character(selected_sample_size)]], 
                aes(sample = Test_Statistics)) +
      geom_qq(col = "blue", shape = 20) +
      geom_qq_line(col = "darkgreen", linetype = 2) +
      labs(title = bquote(italic(L) == .(L)),
           x = "Theoretical Quantiles",
           y = "Sample Quantiles") +
      theme(plot.title = element_text(hjust = 0.5))#titulo centrado

    
    ggplot_list[[as.character(L)]] <- p
  } else {
    warning(paste("No size sample", selected_sample_size, " L =", L))
  }
}
combined_plot1 <- wrap_plots(ggplot_list, ncol = 2, nrow = 2) +
  plot_layout(guides = "collect")

print(combined_plot1)


# }


```

After verifying the normality of the data, we proceeded to explore the capabilities of the proposed test in terms of size and power.
Under $\mathcal{H}_0$, the distribution of test statistic is asymptotically normal. Therefore, the $p$-value  is calculated as $2\Phi(-\mid \varepsilon\mid)$, where  $\Phi$ is the standard Gaussian cumulative distribution and $\varepsilon$ is the standardized test statistic given by:
$$
\varepsilon=\frac{\tilde{H}-H_{\Gamma_{\text{SAR}}}}{\hat\sigma}.
$$

We considered nominal levels of \SI{1}{\percent}, \SI{5}{\percent}, and \SI{10}{\percent}. 
In reference to the size, $1000$ simulations have been used for various sample sizes with  satisfactory results. In all cases, the nominal level has been achieved. 
The power, in general, improves with the increase in the sample size and the number of looks. 
The results are presented in Table&nbsp;\ref{tab:table_size_power}.

```{r Simulated_error_type_I, echo=FALSE, message=FALSE}
set.seed(1234567890, kind = "Mersenne-Twister")



# 
calculate_p_value <- function(test_statistic, mu_W, sigma_W, alpha_nominal) {
  epsilon <- test_statistic / sigma_W
  p_value <- 2 * (1 - pnorm(abs(epsilon)))
  
  return(p_value < alpha_nominal)  
}

R <- 100
mu <- 1
B <- 1
L_values <- c( 3, 5, 8, 11)
sample_sizes <- c( 25, 49, 81, 121)
alpha_nominals <- c(0.01, 0.05, 0.1)

results <- data.frame()

for (L in L_values) {
  for (alpha_nominal in alpha_nominals) {
    TestStatistics <- NULL
    mean_entropy <- numeric(length(sample_sizes))
    sd_entropy <- numeric(length(sample_sizes))
    
    for (s in sample_sizes) {
      TestStat <- numeric(R)
      
      for (r in 1:R) {
        z <- gamma_sar_sample(L, mu, s)
        TestStat[r] <- bootstrap_correa_estimator_log_mean(z, B) - (L - log(L) + lgamma(L) + (1 - L) * digamma(L))
      }
      mean_entropy[sample_sizes == s] <- mean(TestStat)
      sd_entropy[sample_sizes == s] <- sd(TestStat)
      
      TestStatistics <- rbind(TestStatistics, data.frame("L" = rep(L, R), "Sample_Size" = rep(s, R), "Test_Statistics" = TestStat))
    }
    
    mu_W <- mean_entropy
    sigma_W <- sqrt(sd_entropy^2)
    
    p_values <- apply(TestStatistics, 1, function(row) {
      calculate_p_value(row["Test_Statistics"], mu_W[sample_sizes == row["Sample_Size"]], sigma_W[sample_sizes == row["Sample_Size"]], alpha_nominal)
    })
    
    result <- data.frame("L" = TestStatistics$L, "Sample_Size" = TestStatistics$Sample_Size, "Alpha_Nominal" = alpha_nominal, "P_Value" = p_values)
    results <- rbind(results, result)
  }
}




save(results, file = "./Data/type_I_results1.Rdata")

```

```{r Simulated_power, echo=FALSE, message=FALSE}

set.seed(1234567890, kind = "Mersenne-Twister")


calculate_p_value <- function(test_statistic, mu_W, sigma_W) {
  epsilon <- test_statistic  / sigma_W
  p_value <- 2 * (1 - pnorm(abs(epsilon)))
  
  return(p_value)
}


calculate_type_II_error_rate <- function(p_values, alpha_nominal) {
  type_II_error_rate <- sum(p_values >= alpha_nominal) / length(p_values)
  return(type_II_error_rate)
}

# Function to calculate power for different L values and alpha_nominals
calculate_power <- function(R, mu, L_values, B, sample_sizes, alpha_nominals) {
  results <- data.frame()
  
  for (L in L_values) {
    for (alpha_nominal in alpha_nominals) {
      TestStatistics <- list()
      mean_entropy <- numeric(length(sample_sizes))
      sd_entropy <- numeric(length(sample_sizes))
      
      for (s in sample_sizes) {
        TestStat <- numeric(R)
        
        for (r in 1:R) {
          z <- gi0_sample(mu, -5, L, s)
          TestStat[r] <- bootstrap_correa_estimator_log_mean(z, B) - (L - log(L) + lgamma(L) + (1 - L) * digamma(L))
        }
        
        mean_entropy[sample_sizes == s] <- mean(TestStat)
        sd_entropy[sample_sizes == s] <- sd(TestStat)
        
        TestStatistics[[as.character(s)]] <- TestStat
      }
      
      mu_W <-  mean_entropy  
      sigma_W <- sqrt(sd_entropy^2)  
      
      p_values <- lapply(TestStatistics, function(TestStat) {
        apply(data.frame("Test_Statistics" = TestStat), 1, function(row) {
          calculate_p_value(row["Test_Statistics"], mu_W, sigma_W[as.numeric(names(TestStatistics)) == as.numeric(s)])
        })
      })
      
      type_II_error_rates <- sapply(p_values, function(p_values_for_size) {
        calculate_type_II_error_rate(p_values_for_size, alpha_nominal)
      })
      
      power <- 1 - type_II_error_rates
      
      result_row <- data.frame(
        L = L,
        alpha_nominal = alpha_nominal,
        Sample_Size = sample_sizes,
        power = power
      )
      
      results <- rbind(results, result_row)
    }
  }
  
  return(results)
}


R <- 100
mu <- 1
L_values <- c(3, 5,  8, 11)
B <- 1
sample_sizes <- c(25, 49, 81, 121)
alpha_nominals <- c(0.01, 0.05, 0.1)

results_power <- calculate_power(R, mu, L_values, B, sample_sizes, alpha_nominals)
save(results_power, file = "./Data/results_power1.Rdata")

```

```{r Table_size_and_power, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#cat("\\setlength{\\tabcolsep}{4pt}\n")

load("./Data/type_I_results1.Rdata")


type_I_error_rates <- tapply(results$P_Value, INDEX = list(results$L, results$Sample_Size, results$Alpha_Nominal), FUN = function(p_values) {
  sum(p_values) / length(p_values)  
})


type_I_error_results <- as.data.frame(as.table(type_I_error_rates))

colnames(type_I_error_results) <- c("L", "Sample_Size", "Alpha_Nominal", "Type_I_Error_Rate")

spread_results <- spread(type_I_error_results, key = Alpha_Nominal, value = Type_I_Error_Rate)


spread_results <- spread_results %>% 
  select(L, Sample_Size, `0.01`, `0.05`, `0.1`)

load("./Data/results_power1.Rdata")


summary_stats <- results_power %>%
  spread(key = alpha_nominal, value = power) %>%
  select(L, Sample_Size, `0.01`, `0.05`, `0.1`)


combined_results <- merge(spread_results, summary_stats, by = c("L", "Sample_Size"))
combined_results <- combined_results %>% 
  arrange(L, Sample_Size)

combined_results <- combined_results %>% 
  select(L, Sample_Size, `0.01.x`, `0.05.x`, `0.1.x`, `0.01.y`, `0.05.y`, `0.1.y`)


colnames(combined_results) <- c("$L$", "$n$", "$1\\%$", "$5\\%$", "$10\\%$", "$1\\%$", "$5\\%$", "$10\\%$")

# LaTeX notation 
combined_results[] <- lapply(combined_results, function(x) {
  if (is.numeric(x)) {
    if (all(x %% 1 == 0)) {
      formatted_numbers <- sprintf("$%d$", x)
    } else {
      formatted_numbers <- ifelse(x < 0, sprintf("$%.3f$", x), sprintf("$\\phantom{-}%.3f$", x))
    }
    return(formatted_numbers)
  } else {
    return(x)
  }
})


table_combined_result <- knitr::kable(
  combined_results,
  caption = "Size and Power of the proposed test.",
  format = "latex",
  booktabs = TRUE,
  align = "lcccccccc",
  escape = FALSE,
  digits = 3,
  label = "table_size_power",
  centering = FALSE,
  linesep = "",
  table.envir = "table", position = "H")%>%
  add_header_above(c(" ", " ", "Size" = 3,  "Power" = 3)) %>%
  collapse_rows(columns = 1:2, latex_hline = "major", valign = "middle") %>%
  row_spec(0,  align = "c")

print(table_combined_result)



```




# Conclusion {#sec:conclusion} 




